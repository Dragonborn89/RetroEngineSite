<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RetroEngine — Dev Log 2: Building the Foundation</title>
<link rel="stylesheet" href="../style.css">
</head>
<body>

<header>
  <h1>Dev Log 2: Building the Foundation</h1>
  <nav>
    <a href="/">Home</a>
    <a href="/devlog/">Dev Log</a>
    <a href="/about/">About</a>
  </nav>
</header>

<main>

  <section>
    <h2>Part I — Solid Ground</h2>
    <p>
      After bringing the CRT system to life, I shifted focus to structure. How everything underneath it actually works. 
      RetroEngine needed more than just a display; it needed a framework that could scale. 
      The goal was clear: build a clean, modular core that could render both 3D and 2D scenes through the same signal path.
    </p>

    <p>
      I introduced an ECS-style layout for all scene data, starting with <code>Components.h</code> for transforms and renderables, 
      <code>Material.h</code> for shading data, and <code>GpuMesh.h</code> to define how geometry lives on the GPU. 
      It’s not a full ECS engine, just structured enough to keep data predictable and render paths isolated.
    </p>
  </section>

  <section>
    <h2>Part II — Light and Form</h2>
    <p>
      The next step was verifying the math. The first procedural cube mesh finally rendered under 
      proper Blinn–Phong lighting, using inverse-transpose normal transforms for correct world-space shading. 
      A simple test scene proved everything worked end-to-end: geometry buffers, camera transforms, 
      constant buffers, and lighting all talking through DirectX 12.
    </p>

    <p>
      Every pixel rendered here flows through the same HDR render target that the CRT system consumes. 
      Instead of a separate path, the 3D renderer now acts as a signal generator, feeding light data into the analog simulation.
    </p>

    <video controls autoplay muted loop>
      <source src="../assets/videos/devlog02_01.mp4" type="video/mp4">
    </video>
  </section>

  <section>
    <h2>Part III — Preparing for 2D</h2>
    <p>
      With the 3D foundation stable, I’m now working on <strong>SpriteRenderer2D</strong>, a new module built on the same HDR target. 
      It’ll handle orthographic cameras, nearest-neighbor textures, and parallax layers like the SNES and Genesis era.
      The idea is simple but powerful: both the 3D and 2D systems feed the same CRT simulation, 
      producing one unified analog signal regardless of source.
    </p>

    <ul>
      <li>Pure DirectX 12 (no D2D or legacy interop)</li>
      <li>Orthographic camera for pixel-art accuracy</li>
      <li>Layered sprite system with parallax scrolling</li>
      <li>Shared HDR signal with the 3D renderer</li>
    </ul>

    <p>
      The long-term vision is modularity; each renderer acting as its own self-contained signal generator, 
      completely independent but visually cohesive under the CRT stage. 
      Every piece of light that reaches the screen should feel earned, not composited.
    </p>
  </section>

  <section class="summary">
    <h2>Closing Thoughts</h2>
    <p>
      This stage was about order. The chaos of early experiments is now structured: clean data flow, 
      consistent transforms, and a working 3D test scene that breathes through simulated glass. 
      Next, I’ll shift focus to SpriteRenderer2D, giving RetroEngine its first true sense of motion and parallax.
    </p>

    <img src="../assets/images/devlog02_01.jpg" alt="RetroEngine cube lighting test">
  </section>

</main>

<footer>
  <p>© 2025 RetroEngine</p>
</footer>

</body>
</html>